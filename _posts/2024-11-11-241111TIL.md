---
layout : single
title: "2024-11-11 TIL"
categories : TIL
tag: ["241111",blog,TIL]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---


# TIL: 선형 회귀 분석 (Linear Regression)

## 📌 오늘 배운 내용
선형 회귀는 독립 변수와 종속 변수 간의 선형 관계를 모델링하는 기법입니다. 머신러닝의 기초가 되는 중요한 알고리즘입니다.

## 1. 단순 선형 회귀 (Simple Linear Regression)

### 개념
- 하나의 독립 변수(X)로 종속 변수(y)를 예측하는 방법
- y = wx + b 형태의 직선 방정식을 찾는 것이 목표
  - w: 가중치(weight) 또는 기울기(slope)
  - b: 편향(bias) 또는 y절편(intercept)

### 구현 예시
```python
import pandas as pd
from sklearn.linear_model import LinearRegression

# 데이터 준비
dataset = pd.read_csv('LinearRegressionData.csv')
X = dataset['hour'].values
y = dataset['score'].values

# 모델 학습
reg = LinearRegression()
reg.fit(X.reshape(-1, 1), y)

# 예측
prediction = reg.predict([[9]])  # 9시간 공부했을 때의 예상 점수
```

### 학습 내용
- `.reshape(-1, 1)`이 필요한 이유: scikit-learn은 2D 배열 형태의 입력을 요구
- `.fit()`은 모델 학습을, `.predict()`는 예측을 수행
- `reg.coef_`로 기울기, `reg.intercept_`로 y절편 확인 가능

## 2. 데이터셋 분리

### 개념
- 과적합(Overfitting) 방지를 위해 데이터를 훈련셋과 테스트셋으로 분리
- 보통 8:2 또는 7:3 비율로 분리

### 구현
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,     # 테스트셋 20%
    random_state=0     # 재현성을 위한 시드값
)
```

## 3. 경사 하강법 (Gradient Descent)

### 개념
- 비용 함수(Cost Function)를 최소화하는 파라미터를 찾는 최적화 알고리즘
- 학습률(Learning Rate)에 따라 수렴 속도와 정확도가 달라짐

### 구현
```python
from sklearn.linear_model import SGDRegressor

sgd = SGDRegressor(
    max_iter=200,      # 최대 반복 횟수
    eta0=1e-4,        # 학습률
    random_state=0
)
```

## 4. 다중 선형 회귀 (Multiple Linear Regression)

### 개념
- 여러 개의 독립 변수로 종속 변수를 예측
- 범주형 변수는 원-핫 인코딩으로 처리 필요

### 원-핫 인코딩 예시
```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

ct = ColumnTransformer(
    transformers=[('encoder', OneHotEncoder(drop='first'), [2])],
    remainder='passthrough'
)
```

## 5. 모델 평가 지표

### MAE (Mean Absolute Error)
- 실제값과 예측값의 차이 절대값의 평균
- 이해하기 쉽고 이상치의 영향이 적음

### MSE (Mean Squared Error)
- 실제값과 예측값의 차이 제곱의 평균
- 큰 오차에 더 민감하게 반응

### RMSE (Root Mean Squared Error)
- MSE의 제곱근
- 실제 값과 같은 단위로 해석 가능

### R² (결정계수)
- 모델이 데이터의 분산을 얼마나 설명하는지 나타냄
- 1에 가까울수록 좋은 모델

## 🤔 오늘의 고민
1. 언제 단순 선형 회귀와 다중 선형 회귀를 선택해야 할까?
2. 과적합을 방지하기 위한 다른 방법은 무엇이 있을까?
3. 각 평가 지표의 특성에 따른 적절한 사용 상황은?

## 📚 참고자료
- scikit-learn 공식 문서
- Python Machine Learning (Sebastian Raschka)

## 💡 다음에 학습할 내용
- 다항 회귀 (Polynomial Regression)
- 정규화 (Regularization)
- 교차 검증 (Cross Validation)